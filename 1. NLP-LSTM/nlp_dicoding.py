# -*- coding: utf-8 -*-
"""NLP-dicoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b761H8O1l8Iv7jK5S29rTnVp1mzZxFia

#Profile Submission

> Nama : Hamidan Z Wijasena

> Email : hamidanzaneddinewijasena@gmail.com

#Submission-NLP: Klasifikasi Teks pada Dataset BBC News menggunakan metode LSTM
Requirements 

1. Dataset yang akan dipakai bebas, namun minimal memiliki 1000 sampel. 
2. Harus menggunakan LSTM dalam arsitektur model.
3. Harus menggunakan model sequential.
4. Harus menggunakan Embedding.
5. Validation set sebesar 20% dari total dataset.
6. Harus menggunakan fungsi tokenizer. 
7. Akurasi dari model 80%
8. Mengimplementasikan callback.
9. Membuat plot loss dan akurasi pada saat training dan validation.

> Dataset : https://www.kaggle.com/hgultekin/bbcnewsarchive
"""

#Import dataset dari gdrive

from google.colab import drive
drive.mount('/content/drive/')

# Import Library yang akan digunakan

import pandas as pd
import nltk, os, re, string
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, Embedding
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn

# Import Dataset BBCNews

dataframe = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Latihan/NLP/bbc-news-data.csv', sep='\t')
dataframe.head(11)

# Total data

dataframe.shape

dataframe.category.value_counts()

# PLot persebaran data

plt.figure(figsize = (12, 6))
sns.countplot(dataframe.category)

# Hapus kolom yang tidak digunakan

dataframe = dataframe.drop(columns = ['filename'])
dataframe.head(11)

"""#Pre-Processing Dataset

### https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html
"""

# lowercasing

dataframe.title = dataframe.title.apply(lambda x: x.lower())
dataframe.content = dataframe.content.apply(lambda x: x.lower())

# Menghapus tanda baca

def cleaner(data):
    return(data.translate(str.maketrans('','', string.punctuation)))
    dataframe.title = dataframe.title.apply(lambda x: cleaner(x))
    dataframe.content = dataframe.content.apply(lambda x: lem(x))

# Menghapus angka

def rem_numbers(data):
    return re.sub('[0-9]+','',data)
    dataframe['title'].apply(rem_numbers)
    dataframe['content'].apply(rem_numbers)

# Lematization

lemmatizer = WordNetLemmatizer()

def lem(data):
    pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}
    return(' '.join([lemmatizer.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(data.split())]))
    dataframe.title = dataframe.title.apply(lambda x: lem(x))
    dataframe.content = dataframe.content.apply(lambda x: lem(x))

# Menghapus stopword

st_words = stopwords.words('english')
def stopword(data):
    return(' '.join([w for w in data.split() if w not in st_words ]))
    dataframe.title = dataframe.title.apply(lambda x: stopword(x))
    dataframe.content = dataframe.content.apply(lambda x: lem(x))

dataframe.head(11)

# One-hot-encoding

category = pd.get_dummies(dataframe.category)
dataframe = pd.concat([dataframe, category], axis = 1)
dataframe = dataframe.drop(columns = 'category')
dataframe.head(11)

# Ubah nilai pada dataframe menjadi numpy array

news = dataframe['title'].values + '' + dataframe['content'].values
label = dataframe[['business', 'entertainment', 'politics', 'sport', 'tech']].values

label

# Split data menjadi data training dan data validation

news_train, news_test, label_train, label_test = train_test_split(news, label, test_size=0.2, shuffle=True)

# Tokenizer
 
tokenizer = Tokenizer(num_words=5000, oov_token='x', filters='!"#$%&()*+,-./:;<=>@[\]^_`{|}~ ')
tokenizer.fit_on_texts(news_train) 
tokenizer.fit_on_texts(news_test)
 
sekuens_train = tokenizer.texts_to_sequences(news_train)
sekuens_test = tokenizer.texts_to_sequences(news_test)
 
padded_train = pad_sequences(sekuens_train) 
padded_test = pad_sequences(sekuens_test)

"""# Model Building """

# Model

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy',)
model.summary()

# Callback

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.90 and logs.get('val_accuracy')>0.85):
      self.model.stop_training = True
      print("\nThe accuracy of the training set has reached > 90% and the validation set has reached > 85%!")
callbacks = myCallback()

# Model Fit

history = model.fit(padded_train, label_train, epochs = 200, 
                    validation_data = (padded_test, label_test), 
                    verbose = 1, callbacks = [callbacks], validation_steps = 20)

"""# Model Evaluation"""

# Plot Accuracy

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot Loss

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()