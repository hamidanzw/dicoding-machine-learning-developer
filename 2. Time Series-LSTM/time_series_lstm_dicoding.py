# -*- coding: utf-8 -*-
"""Time-series-LSTM-dicoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bOVR-WQ7wUvBiUnGS5VnFbpen3Zk7cjV

#Profile Submission

> Nama : Hamidan Z Wijasena

> Email : hamidanzaneddinewijasena@gmail.com

#Submission-NLP: Klasifikasi Teks pada Dataset BBC News menggunakan metode LSTM
Requirements 

1. Dataset yang akan dipakai bebas, namun minimal memiliki 10000 sampel.

2. Harus menggunakan LSTM dalam arsitektur model.

3. Validation set sebesar 20% dari total dataset.

4. Model harus menggunakan model sequential.

5. Harus menggunakan Learning Rate pada Optimizer.

6. MAE < 10% skala data.

> Dataset : https://www.kaggle.com/mahirkukreja/delhi-weather-data
"""

#Import dataset dari gdrive

from google.colab import drive
drive.mount('/content/drive/')

# Import Library yang akan digunakan

import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from sklearn.model_selection import train_test_split
from keras.layers import Dense, LSTM

"""# Info Dataset

> https://www.kaggle.com/mahirkukreja/delhi-weather-data
"""

# Import Dataset Time Series

dataframe = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Latihan/Time-Series/testset.csv')
dataframe.head(11)

# Ujung dataset

dataframe.tail(11)

# Total data

dataframe.shape

print(dataframe.columns)
print(dataframe.shape)
print(dataframe.info())

# Persebaran data

dataframe.isnull().sum()

"""# Pre-Processing Dataset
>https://towardsdatascience.com/common-time-series-data-analysis-methods-and-forecasting-models-in-python-f0565b68a3d8
"""

# Mengconvert Format datetime_utc ke Datetime

dataframe['datetime_utc'] = pd.to_datetime(dataframe['datetime_utc'])
dataframe['datetime_utc'].head()

# Mengisi lossdata dengan nilai mean

dataframe[' _tempm'].fillna(dataframe[' _tempm'].mean(), inplace=True)
dataframe = dataframe[['datetime_utc',' _tempm' ]]

# Merubah nama kolom
dataframe.rename(columns={'datetime_utc':'date',' _tempm':'tmp'}, inplace=True)
dataframe.head(11)

# Mengecek data kembali 

dataframe.info()

dataframe2=dataframe[['date','tmp']].copy()
dataframe2.set_index('date', inplace= True)

# Resampling data menjadi mean tiap pada tanggal
dataframe2 =dataframe2.resample('D').mean()
dataframe2.head(6)

dataframe2.isnull().sum()

# Isi data kosong dengan nilai mean

dataframe2['tmp'].fillna(dataframe2['tmp'].mean(), inplace=True)

# Cek data
dataframe2.isnull().sum()

# Visualisasi dataset

plt.figure(figsize=(25, 7))
plt.plot(dataframe2, linewidth=.5)
plt.grid()
plt.title("Delhi Weather")
plt.xlabel('Tanggal')
plt.ylabel('Rata-Rata Suhu')
plt.show()

"""# Model Building"""

# Data Value

date = dataframe['date'].values
temp = dataframe['tmp'].values

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

# Membagi Dataset Training 80% dan Validation 20%

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(temp, date, test_size = 0.2, random_state = 0 , shuffle=False)
print(len(x_train), len(x_test))

# Model NN
from keras.layers import Dense, LSTM

data_train = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=1000)
data_test = windowed_dataset(x_test, window_size=60, batch_size=100, shuffle_buffer=1000)

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=32, kernel_size=5,
                      strides=1, padding="causal",
                      activation="relu",
                      input_shape=[None, 1]),
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

# Model Optimizer

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

Mae = (dataframe2['tmp'].max() - dataframe2['tmp'].min()) * 10/100
print(Mae)

# Callback

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')< Mae):
      print("\nMAE of the model < 10% of data scale")
      self.model.stop_training = True
callbacks = myCallback()

# Fitting Model 

tf.keras.backend.set_floatx('float64')
history = model.fit(data_train ,epochs=100, validation_data=data_test, callbacks=[callbacks])

"""# Model Evaluation"""

# Plot MAE

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('MAE')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

# Plot Loss

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()